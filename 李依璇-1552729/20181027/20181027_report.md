1、experiment部份

* 主要使用 tensorflow.contrib.rnn API，搭建了一个二层的 LSTM 网络，其内部的前馈层，及层间连线未customize，使用API default。任务是识别手写数字图片，raw data是28*28的bitmap，强用LSTM（使用强用是因为，貌似婴儿不是从上到下扫视数字的，而是视野在中心，或者..最吸睛的部分看起...）将二维中一维视作序，分28步输入[28]^T的bit vector。输出一个热编码格式的归一化vector[10]，最后取其中argmax 作为识别结果。由于分类是互斥集，使用此LSTM模型"输出"(learned latent features？）接入分类层，做一次 softmax 回归。这里按常规使用log-like cost，类似交叉熵，后者原理此处不详悉。
* 主要是 复现了一个经典的cnn：letnet5 进行手写识别任务。其包含了两个卷积层及[relu](http://shuokay.com/2016/10/01/why-relu-work/)和降取样操作。最后一部份是[FC层](https://www.zhihu.com/question/41037974)和softmax

2、paper部份

主要涉过两篇。

两篇论文有殊途同归的motivation 和  core-idea。

motivation：reduce human force on extracting the actual anomalies of interest.

core idea：adjust the anomaly ranking after every analyst interaction, ideally moving anomalies of interest closer to the top.



