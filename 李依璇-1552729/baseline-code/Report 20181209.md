#### Report 20181206

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180501225334923?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2V4dHJlbWViaW5nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



##### Isolation Forest

In 2010, Professor Zhi-Hua Zhou of Nanjing University proposed an anomaly detection algorithm, Isolation Forest, which is very practical in the industry. The algorithm has good effect and time efficiency. It can effectively process high-dimensional data and massive data. 

[1] Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. “Isolation forest.” Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on. IEEE, 2008.

[2] Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. “Isolation-based anomaly detection.” ACM Transactions on Knowledge Discovery from Data (TKDD) 6.1 (2012): 3.



[^description]: table 1-1：the best train_validation_result for first 2 and first 12  type of KPIs anomalies detection 

| KPI_used | accuracy | f1_score | precision | recall | special setting                                              |
| -------- | -------- | -------- | --------- | ------ | ------------------------------------------------------------ |
| first 2  | 0.7940   | 0.0147   | 0.0076    | 0.1753 | moderate under-sampling on normal and oversampling on anomalies |
| first 12 | 0.7349   | 0.0545   | 0.0159    | 1.0000 | moderate under-sampling on normal data                       |

The result  leads to some conjectures.

First of all,  sampling helps improve the basic isolation forest's performance .

Secondly, the isolation forest's performance is stable, the more types of KPIs it learns, the better. 

Lastly, under the experiment settings, i_forest tends to make quite a lot false anomalies warms, which may need further skills to fix up.





[^description]: table 1-2：the train_validation_result after more special settings involved

| KPI_used | accuracy | f1_score | precision | recall | special setting                                              |
| -------- | -------- | -------- | --------- | ------ | ------------------------------------------------------------ |
| all      | 0.5088   | 0.0825   | 0.1091    | 0.0663 | shuffled and oversampling on anomalies                       |
| not all  | 0.3681   | 0.0977   | 0.1717    | 0.0683 | added feature to represent the former prediction score in validation / previous points' label in training |
| not all  | 0.3701   | 0.1021   | 0.1787    | 0.0715 | added feature to represent  KPI type(number)                 |
| not all  | 0.3914   | 0.1300   | 0.2266    | 0.0911 | 0.1 bigger fraction for training  with origin validation/fit ratio (0.2/0.8:0.9) |



[^description]: bellow shows detailed experiment records which reveals metrics, data construction, time cost in building off-line models and predicting online. 



```python

(train_validation 1 to 2) model id: 0, accuracy: 0.7929, f1 score: 0.0133, precision: 0.0069, recall: 0.1598
KPI samples: total= 110204
normal samples: total= 110010  normal/total = 0.9982396283256506
anomaly samples: total= 194  anomaly/total = 0.0017603716743493883
train samples: total= 88008 train#/sample#= 0.7985917026605205
validation samples  : total= 22196 validation#/sample#= 0.20140829733947951 anomaly/normal= 0.008817380238160167
model building time: 5.372827529907227
data predict time: 1.0071754455566406
(train_validation 1 to 2) model id: 0, accuracy: 0.0028, f1 score: 0.0038, precision: 0.0019, recall: 1.0000
batch:  13
samples: total= 27551
anomalies: total= 53
normals: total= 27498
results: true positives= 50 true negatives= 24 false positives= 25926 false negatives= 0
data predict time: 2.2553584575653076

(train_validation 1 to 2) model id: 0, accuracy: 0.7940, f1 score: 0.0147, precision: 0.0076, recall: 0.1753
ensemble_data=easyensemble(train_data, 0.06,n_subsets=N_SUBSETS)
N_SUBSETS=1
x,y=sm(x,y)
outliers_fraction=len(X_outliers)/(len(X_outliers)+len(X_train))
i_forest=IsolationForest(behaviour='new',contamination=outliers_fraction,random_state=42)
(test_validation 1 to 2) model id: 0, accuracy: 0.0042, f1 score: 0.0041, precision: 0.0020, recall: 1.0000
samples: total= 27551
anomalies: total= 53
normals: total= 27498
results: true positives= 53 true negatives= 57 false positives= 25890 false negatives= 0




(train_validation 1 to 2) model id: 0, accuracy: 0.6238, f1 score: 0.1924, precision: 0.2000, recall: 0.1854
ensemble_data=easyensemble(train_data, 0.06,n_subsets=N_SUBSETS)
N_SUBSETS=1
outliers_fraction=len(X_outliers)/(len(X_outliers)+len(X_train))
i_forest=IsolationForest(behaviour='new',contamination=outliers_fraction,random_state=42)
(test_validation 1 to 2) model id: 0, accuracy: 0.0030, f1 score: 0.0030, precision: 0.0015, recall: 1.0000
samples: total= 27551
anomalies: total= 42
normals: total= 27509
results: true positives= 39 true negatives= 40 false positives= 25921 false negatives= 0
predict time: 1.8197309970855713


@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

(train_validation 1 to 12) model id: 0, accuracy: 0.7349, f1 score: 0.0545, precision: 0.0375, recall: 0.1000
KPI samples: total= 849140
normal samples: total= 835322  normal/total = 0.9837270650305014
anomaly samples: total= 13818  anomaly/total = 0.01627293496949855
train samples: total= 668257 train#/sample#= 0.786980945427138
validation samples  : total= 180883 validation#/sample#= 0.21301905457286197 anomaly/normal= 0.08271032232963219
model building time: 52.30820274353027
data predict time: 8.638025283813477
(test_validation 1 to 12) model id: 0, accuracy: 0.0160, f1 score: 0.0313, precision: 0.0159, recall: 1.0000
data predict time: 12.717535018920898
batch:  106
samples: total= 212285
anomalies: total= 3371
normals: total= 208914
results: true positives= 3368 true negatives= 15 false positives= 208617 false negatives= 0



(train_validation 1 to 12) model id: 0, accuracy: 0.7699, f1 score: 0.0591, precision: 0.0368, recall: 0.1503
ensemble_data=easyensemble(train_data, 0.01,n_subsets=1)
KPI samples: total= 19299
normal samples: total= 19106  normal/total = 0.9899994818384372
anomaly samples: total= 193  anomaly/total = 0.010000518161562775
train samples: total= 15284 train#/sample#= 0.7919581325457278
validation samples  : total= 4015 validation#/sample#= 0.20804186745427225 anomaly/normal= 0.050497121925693354
model building time: 0.9207198619842529
data predict time: 0.18961286544799805
KPI ID: 1to12with0.25and5, accuracy: 0.0030, f1 score: 0.0041, precision: 0.0021, recall: 1.0000
samples: total= 27551
anomalies: total= 54
normals: total= 27497
results: true positives= 54 true negatives= 25 false positives= 25921 false negatives= 0

(train_validation 1 to 12) model id: 0, accuracy: 0.7802, f1 score: 0.0342, precision: 0.0192, recall: 0.1587
ensemble_data=easyensemble(train_data, 0.005,n_subsets=1)
KPI samples: total= 37800
normal samples: total= 37611  normal/total = 0.995
anomaly samples: total= 189  anomaly/total = 0.005
train samples: total= 30088 train#/sample#= 0.795978835978836
validation samples  : total= 7712 validation#/sample#= 0.20402116402116402 anomaly/normal= 0.025122956267446497
model building time: 1.8607401847839355
data predict time: 0.37674522399902344
(test_validation 1 to 12) model id: 0, accuracy: 0.0037, f1 score: 0.0041, precision: 0.0020, recall: 0.9815
batch:  13
samples: total= 27551
anomalies: total= 58
normals: total= 27493
results: true positives= 53 true negatives= 42 false positives= 25904 false negatives= 1

(train_validation 1 to 12) model id: 0, accuracy: 0.7892, f1 score: 0.0219, precision: 0.0118, recall: 0.1592
ensemble_data=easyensemble(train_data, 0.003,n_subsets=1)
KPI samples: total= 67000
normal samples: total= 66799  normal/total = 0.997
anomaly samples: total= 201  anomaly/total = 0.003
train samples: total= 53439 train#/sample#= 0.7975970149253732
validation samples  : total= 13561 validation#/sample#= 0.20240298507462687 anomaly/normal= 0.015044910179640719
model building time: 3.2516772747039795
data predict time: 0.595341682434082

(test_validation 1 to 12) model id:0, accuracy: 0.0030, f1 score: 0.0031, precision: 0.0015, recall: 1.0000
samples: total= 27551
anomalies: total= 46
normals: total= 27505
results: true positives= 40 true negatives= 39 false positives= 25921 false negatives= 0
data predict time: 1.8431107997894287
```





### Report 20181209

##### Isolation Forest

Mainly introduce small adjustment like 

* added kpi type

* added features: previous state,kpi type,[window statistic like mean,min,max]( https://www.jianshu.com/p/4ece90357020)
* higher train/test set ratio

to boost i_Forest's performance on our anomaly detection problem, which is revealed by  validation set

[^description]: table 1-2：the train_validation_result after involved adjustments

| KPI_used | accuracy | f1_score | precision | recall | special setting                                              |
| -------- | -------- | -------- | --------- | ------ | ------------------------------------------------------------ |
| all      | 0.5088   | 0.0825   | 0.1091    | 0.0663 | shuffled and oversampling on anomalies                       |
| not all  | 0.3681   | 0.0977   | 0.1717    | 0.0683 | added feature to represent the former prediction score in validation / previous points' label in training |
| not all  | 0.3701   | 0.1021   | 0.1787    | 0.0715 | added feature to represent  KPI type(number)                 |
| not all  | 0.3914   | 0.1300   | 0.2266    | 0.0911 | 0.1 bigger fraction for training  with origin validation/fit ratio (0.2/0.8:0.9) |



#### One-class SVM

[3] Schölkopf B, Williamson R C, Smola A J, et al. Support vector method for novelty detection[C]//Advances in neural information processing systems. 2000: 582-588.

A natural extension of the support vector algorithm to the case of unlabeled data. In a general sense, one-class svm builds a model that describes the boundary of normal data,  which may leads to many false positive alarms or in contrary.

Linear separable modes in low-dimensional space may be realized by mapping non-linearly to high-dimensional feature space.Kernel function is used to overcome the problem of curse of dimensionality.

```
			oc_svm=OneClassSVM(nu=0.1, kernel="rbf", gamma='scale')
```

$$
k(||x-x_c||)=e^{-(||x-x_c||/2σ)^2}=e^{-γ||x-x_c||^2} ，γ>0
$$

$$
x_c为核函数中心,σ为函数的宽度参数 , 控制函数的径向作用范围
$$



One drawback is time cost. First i tried PCA, but it turns out to be ineffective.So i make use of time series 's seasonality, i pick out one month interval for normal-model-building and the rest normals along with anomalies (undersampled to 1:1) for validation. 

| KPI_used | accuracy | F1_score | precision | recall | time_cost                                                    | settings |
| -------- | -------- | -------- | --------- | ------ | ------------------------------------------------------------ | -------- |
| one      | 0.7236   | 0.1146   | 0.0608    | 1.0000 | model building time: 44.34589171409607    model building time: 995.3211654728312 |          |
| five     | 0.6253   | 0.4983   | 0.3318    | 1.0000 | model building time: 932.9938740730286                       |          |

##### Logistic Regression

[^descrption]: table 2-1: logistic regression algorithm's performance on whole kpi data set

| KPI_used | accuracy | F1__score | precision | recall | settings                         |
| -------- | -------- | --------- | --------- | ------ | -------------------------------- |
| all      | 0.5497   | 0.5262    | 0.5262    | 0.5262 | oversampling+0.2  for validation |

##### Random Forest

[^description]: table 2-1: random forest algorithm's performance on whole kpi data set

| KPI_used | accuracy | f1__score | precision | recall | settings                        |
| -------- | -------- | --------- | --------- | ------ | ------------------------------- |
| all      | 0.6040   | 0.5591    | 0.5591    | 0.9823 | oversampling+0.2 for validation |



